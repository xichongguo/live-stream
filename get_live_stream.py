"""
File: get_live_stream.py
Function:
  - API & whitelist.txt -> group-title="æœ¬åœ°èŠ‚ç›®"ï¼Œå…æ£€ç›´æ¥ä¿ç•™
  - tv.m3u (ä¼˜å…ˆ)ã€æµ·ç‡•.txtã€ç”µè§†å®¶.txt -> è‡ªåŠ¨åˆ†ç±» + æ£€æµ‹ IPv4 + å¯ç”¨æ€§
  - ç™½åå•æºä¸è¿›è¡Œä»»ä½•æ£€æµ‹ï¼ˆåŒ…æ‹¬ IPv6 è¿‡æ»¤ï¼‰
  - è¾“å‡º live/current.m3u8
"""

import requests
import os
import socket
from urllib.parse import unquote, urlparse, parse_qs, urlunparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed


# ================== Configuration ==================
API_URL = "https://lwydapi.xichongtv.cn/a/appLive/info/35137_b14710553f9b43349f46d33cc2b7fcfd"
PARAMS = {
    'deviceType': '1',
    'centerId': '9',
    'deviceToken': 'beb09666-78c0-4ae8-94e9-b0b4180a31be',
    'latitudeValue': '0',
    'areaId': '907',
    'appCenterId': '907',
    'isTest': '0',
    'longitudeValue': '0',
    'deviceVersionType': 'android',
    'versionCodeGlobal': '5009037'
}
HEADERS = {
    'User-Agent': 'okhttp/3.12.12',
}

# Remote sources
REMOTE_WHITELIST_URL = "https://raw.githubusercontent.com/xichongguo/live-stream/main/whitelist.txt"
TV_M3U_URL = "https://raw.githubusercontent.com/wwb521/live/refs/heads/main/tv.m3u"
HAIYAN_TXT_URL = "https://chuxinya.top/f/AD5QHE/%E6%B5%B7%E7%87%95.txt"
DIANSHIJIA_TXT_URL = "https://gitproxy.click/https://github.com/wujiangliu/live-sources/blob/main/dianshijia_10.1.txt"

WHITELIST_TIMEOUT = 15
REQUEST_TIMEOUT = (5, 10)
DEFAULT_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# Channel category rules
CATEGORY_MAP = {
    'å¤®è§†': ['cctv', 'ä¸­å¤®'],
    'å«è§†': [
        'å«è§†', 'æ¹–å—', 'æµ™æ±Ÿ', 'æ±Ÿè‹', 'ä¸œæ–¹', 'åŒ—äº¬', 'å¹¿ä¸œ', 'æ·±åœ³', 'å››å·', 'æ¹–åŒ—', 'è¾½å®',
        'ä¸œå—', 'å¤©æ´¥', 'é‡åº†', 'é»‘é¾™æ±Ÿ', 'å±±ä¸œ', 'å®‰å¾½', 'äº‘å—', 'é™•è¥¿', 'ç”˜è‚ƒ', 'æ–°ç–†',
        'å†…è’™å¤', 'å‰æ—', 'æ²³åŒ—', 'å±±è¥¿', 'å¹¿è¥¿', 'æ±Ÿè¥¿', 'ç¦å»º', 'è´µå·', 'æµ·å—'
    ],
    'åœ°æ–¹': [
        'éƒ½å¸‚', 'æ–°é—»', 'ç»¼åˆ', 'å…¬å…±', 'ç”Ÿæ´»', 'å½±è§†é¢‘é“', 'å½±è§†', 'ç”µè§†å‰§', 'å¨±ä¹',
        'å°‘å„¿', 'å¡é€š', 'ä½“è‚²', 'è´¢ç»', 'çºªå®', 'æ•™è‚²', 'æ°‘ç”Ÿ', 'äº¤é€š', 'æ–‡è‰º', 'éŸ³ä¹',
        'æˆæ›²', 'é«˜å°”å¤«', 'ç½‘çƒ'
    ]
}


# ================== Utility Functions ==================
def is_ipv4_address(ip):
    """Check if the given string is a valid IPv4 address."""
    try:
        socket.inet_pton(socket.AF_INET, ip)
        return True
    except (socket.error, TypeError):
        return False


def get_ip_version(url):
    """
    Resolve domain in URL to IP, return 'ipv4' or 'ipv6'
    Returns 'unknown' if failed.
    """
    try:
        parsed = urlparse(url)
        hostname = parsed.netloc.split(':')[0]  # Remove port
        addr_info = socket.getaddrinfo(hostname, None, family=socket.AF_INET)
        for info in addr_info:
            ip = info[4][0]
            if is_ipv4_address(ip):
                return 'ipv4'
        return 'ipv6'
    except Exception as e:
        print(f"âš ï¸ DNS resolve failed for {url}: {e}")
        return 'unknown'


def is_url_valid(url):
    """
    Check if stream is accessible AND uses IPv4.
    Returns True only if both pass.
    """
    try:
        # Step 1: Check IPv4
        ip_ver = get_ip_version(url)
        if ip_ver != 'ipv4':
            print(f"ğŸš« Not IPv4 ({ip_ver}): {url}")
            return False

        # Step 2: HEAD request
        head = requests.head(
            url,
            timeout=REQUEST_TIMEOUT,
            allow_redirects=True,
            headers=DEFAULT_HEADERS
        )
        success = 200 <= head.status_code < 400
        if success:
            print(f"âœ… Live OK: {url}")
        else:
            print(f"âŒ Stream dead ({head.status_code}): {url}")
        return success

    except Exception as e:
        print(f"âŒ Failed to play {url}: {e}")
        return False


def normalize_url(url):
    """Remove tracking/query params for deduplication."""
    try:
        parsed = urlparse(url.lower())
        safe_params = {}
        unsafe_keys = {'token', 't', 'ts', 'sign', 'auth_key', 'verify', 'session', 'key', 'pwd'}
        for k, v in parse_qs(parsed.query).items():
            if k.lower() not in unsafe_keys:
                safe_params[k] = v[0] if v else ''
        new_query = '&'.join(f"{k}={v}" for k, v in safe_params.items() if v)
        return urlunparse(parsed._replace(query=new_query))
    except:
        return url.lower().split('?')[0]


def merge_and_deduplicate_with_flag(channels):
    """Remove duplicates based on normalized URL (keep first occurrence)"""
    seen = set()
    unique = []
    for item in channels:
        name, url, group, is_whitelist = item
        norm_url = normalize_url(url)
        if norm_url not in seen:
            seen.add(norm_url)
            unique.append(item)
        else:
            print(f"ğŸ” Skipped duplicate: {url}")
    print(f"âœ… After dedup: {len(unique)} unique streams")
    return unique


def categorize_channel(name):
    """Auto categorize channel by name."""
    name_lower = name.lower()
    for category, keywords in CATEGORY_MAP.items():
        for kw in keywords:
            if kw.lower() in name_lower:
                return category
    return "å…¶ä»–"


def load_whitelist_from_remote():
    """Load whitelist -> æœ¬åœ°èŠ‚ç›® (trusted, no test, keep all)"""
    print(f"ğŸ‘‰ Loading trusted whitelist: {REMOTE_WHITELIST_URL}")
    try:
        response = requests.get(REMOTE_WHITELIST_URL, timeout=WHITELIST_TIMEOUT)
        response.raise_for_status()
        lines = response.text.strip().splitlines()
        channels = []

        for line in lines:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            parts = [p.strip() for p in line.split(",", 1)]
            if len(parts) < 2:
                continue
            name, url = parts[0], parts[1]
            if not name or not url or not url.startswith(("http://", "https://")):
                continue
            channels.append((name, url, "æœ¬åœ°èŠ‚ç›®", True))  # is_whitelist=True
            print(f"  â• Whitelist: {name} -> æœ¬åœ°èŠ‚ç›® (trusted, no test)")
        print(f"âœ… Loaded {len(channels)} from whitelist (no test)")
        return channels
    except Exception as e:
        print(f"âŒ Load whitelist failed: {e}")
        return []


def load_tv_m3u():
    """Load tv.m3u (priority source, needs testing)"""
    print(f"ğŸ‘‰ Loading priority source: {TV_M3U_URL}")
    try:
        response = requests.get(TV_M3U_URL, timeout=WHITELIST_TIMEOUT, headers=DEFAULT_HEADERS)
        response.raise_for_status()
        lines = response.text.strip().splitlines()
        channels = []
        current_name = None

        for line in lines:
            line = line.strip()
            if line.startswith("#EXTINF"):
                try:
                    name_part = line.split(",", 1)
                    if len(name_part) > 1:
                        current_name = name_part[1].strip()
                except:
                    current_name = "Unknown"
            elif line.startswith("http"):
                if current_name and line.startswith(("http://", "https://")):
                    category = categorize_channel(current_name)
                    channels.append((current_name, line, category, False))  # need test
                    print(f"  â• tv.m3u: {current_name} -> {category}")
                current_name = None
        print(f"âœ… Loaded {len(channels)} from tv.m3u (will test)")
        return channels
    except Exception as e:
        print(f"âŒ Failed to load tv.m3u: {e}")
        return []


def load_haiyan_txt():
    """Load æµ·ç‡•.txt -> auto categorize (needs testing)"""
    print(f"ğŸ‘‰ Loading æµ·ç‡•.txt: {HAIYAN_TXT_URL}")
    try:
        decoded_url = unquote(HAIYAN_TXT_URL)
        response = requests.get(decoded_url, timeout=WHITELIST_TIMEOUT, headers=DEFAULT_HEADERS)
        response.raise_for_status()
        response.encoding = 'utf-8'
        lines = response.text.strip().splitlines()
        channels = []

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line or line.startswith("#") or line.startswith("æ›´æ–°æ—¶é—´") or line.startswith("TV"):
                continue
            if "," not in line:
                continue
            try:
                name, url = map(str.strip, line.split(",", 1))
                if not name or not url or not url.startswith(("http://", "https://")):
                    continue
                category = categorize_channel(name)
                channels.append((name, url, category, False))
                print(f"  â• æµ·ç‡•.txt: {name} -> {category}")
            except Exception as e:
                print(f"âš ï¸ Parse failed at line {line_num}: {line} | {e}")
        print(f"âœ… Loaded {len(channels)} from æµ·ç‡•.txt (will test)")
        return channels
    except Exception as e:
        print(f"âŒ Load æµ·ç‡•.txt failed: {e}")
        return []


def load_dianshijia_txt():
    """Load ç”µè§†å®¶.txt -> auto categorize (needs testing)"""
    print(f"ğŸ‘‰ Loading ç”µè§†å®¶.txt: {DIANSHIJIA_TXT_URL}")
    try:
        raw_url = DIANSHIJIA_TXT_URL.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
        response = requests.get(raw_url, timeout=WHITELIST_TIMEOUT, headers=DEFAULT_HEADERS)
        response.raise_for_status()
        response.encoding = 'utf-8'
        lines = response.text.strip().splitlines()
        channels = []

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line or line.startswith("#") or line.startswith("æ›´æ–°æ—¶é—´") or line.startswith("TV"):
                continue
            if "," not in line:
                continue
            try:
                name, url = map(str.strip, line.split(",", 1))
                if not name or not url or not url.startswith(("http://", "https://")):
                    continue
                category = categorize_channel(name)
                channels.append((name, url, category, False))
                print(f"  â• ç”µè§†å®¶.txt: {name} -> {category}")
            except Exception as e:
                print(f"âš ï¸ Parse failed at line {line_num}: {line} | {e}")
        print(f"âœ… Loaded {len(channels)} from ç”µè§†å®¶.txt (will test)")
        return channels
    except Exception as e:
        print(f"âŒ Load ç”µè§†å®¶.txt failed: {e}")
        return []


def filter_and_test_streams(channels, max_workers=10):
    """Concurrently test non-whitelist streams for IPv4 + availability."""
    print(f"ğŸ” Testing {len(channels)} untrusted streams (IPv4 + alive check)...")
    valid_channels = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_item = {
            executor.submit(is_url_valid, url): (name, url, group, False)
            for name, url, group, _ in channels
        }

        for future in as_completed(future_to_item):
            (name, url, group, _) = future_to_item[future]
            try:
                if future.result():
                    valid_channels.append((name, url, group, False))
            except Exception as e:
                print(f"âš ï¸ Exception during test {url}: {e}")

    print(f"âœ… After testing: {len(valid_channels)} valid IPv4 streams")
    return valid_channels


def generate_m3u8_content(dynamic_url, channels):
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    lines = [
        "#EXTM3U",
        f"# Generated at: {now}",
        "x-tvg-url=\"https://epg.51zmt.top/xmltv.xml\""
    ]

    if dynamic_url:
        lines.append('#EXTINF:-1 tvg-name="è¥¿å……ç»¼åˆ" group-title="æœ¬åœ°èŠ‚ç›®",è¥¿å……ç»¼åˆ')
        lines.append(dynamic_url)

    for name, url, group, _ in channels:
        lines.append(f'#EXTINF:-1 tvg-name="{name}" group-title="{group}",{name}')
        lines.append(url)

    return "\n".join(lines) + "\n"


def get_dynamic_stream():
    """Fetch dynamic stream from API."""
    print("ğŸ‘‰ Fetching dynamic stream from API...")
    try:
        response = requests.get(API_URL, params=PARAMS, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        if 'data' in data and 'm3u8Url' in data['data']:
            url = data['data']['m3u8Url']
            if is_url_valid(url):
                print(f"âœ… Dynamic stream OK: {url}")
                return url
            else:
                print(f"âŒ Stream not accessible: {url}")
        else:
            print("âŒ m3u8Url not found in API response")
    except Exception as e:
        print(f"âŒ API request failed: {e}")
    return None


def main():
    print("ğŸš€ Starting playlist generation...")
    os.makedirs('live', exist_ok=True)
    print("ğŸ“ Ensured live/ directory")

    dynamic_url = get_dynamic_stream()
    all_channels = []

    # é¡ºåºåŠ è½½ï¼šç™½åå•ä¼˜å…ˆ + å…¶ä»–æº
    all_channels.extend(load_tv_m3u())                    # éœ€è¦æ£€æµ‹
    all_channels.extend(load_whitelist_from_remote())     # å…æ£€
    all_channels.extend(load_haiyan_txt())                # éœ€è¦æ£€æµ‹
    all_channels.extend(load_dianshijia_txt())            # éœ€è¦æ£€æµ‹

    print(f"ğŸ“¥ Total raw streams: {len(all_channels)}")

    # å»é‡ï¼ˆåŸºäº URL å½’ä¸€åŒ–ï¼‰
    unique_channels = merge_and_deduplicate_with_flag(all_channels)

    # åˆ†æµå¤„ç†
    trusted = [item for item in unique_channels if item[3]]      # is_whitelist=True
    untrusted = [item for item in unique_channels if not item[3]]

    print(f"ğŸ›¡ï¸  {len(trusted)} trusted channels (from whitelist, skip all tests)")
    print(f"ğŸ” {len(untrusted)} untrusted channels (testing IPv4 + availability)...")

    # ä»…å¯¹éç™½åå•æºè¿›è¡Œæ£€æµ‹
    valid_untrusted = filter_and_test_streams(untrusted, max_workers=15)

    # åˆå¹¶ï¼šç™½åå• + æ£€æµ‹é€šè¿‡çš„
    final_channels = trusted + valid_untrusted
    print(f"âœ… Final playlist size: {len(final_channels)} channels")

    # ç”Ÿæˆ M3U8ï¼ˆå»æ‰ç¬¬å››ä¸ªå­—æ®µï¼‰
    m3u8_content = generate_m3u8_content(dynamic_url, final_channels)

    # å†™å…¥æ–‡ä»¶
    output_path = 'live/current.m3u8'
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(m3u8_content)
        print(f"ğŸ‰ Successfully generated: {output_path}")
    except Exception as e:
        print(f"âŒ Write failed: {e}")
        return

    if not os.path.exists('.nojekyll'):
        open('.nojekyll', 'w').close()
        print("ğŸ“„ Created .nojekyll")

    print("âœ… All tasks completed!")


if __name__ == "__main__":
    main()
