"""
ç›´æ’­æºèšåˆè„šæœ¬
åŠŸèƒ½ï¼š
  - API & whitelist.txt -> group-title="æœ¬åœ°èŠ‚ç›®"ï¼Œå…æ£€ç›´æ¥ä¿ç•™
  - tv.m3uã€rihou.ccã€æµ·ç‡•.txt -> è‡ªåŠ¨åˆ†ç±» + å¯ç”¨æ€§æ£€æµ‹
  - ç™½åå•æºä¸æ£€æµ‹ï¼Œå…¶ä»–æºè¿›è¡Œå¿«é€Ÿå¯ç”¨æ€§æ£€æŸ¥
  - å¢åŠ çœä»½åˆ†ç±»ï¼Œè¿‡æ»¤å›½å¤–é¢‘é“ï¼ˆä¿ç•™æ¸¯æ¾³å°ï¼‰
  - 'èµ›äº‹å’ªå’•' åˆ†ç±»ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
  - åŒåé¢‘é“ä¼˜å…ˆä¿ç•™ IPv4 æºï¼Œå»é™¤å¤±æ•ˆæº
  - è¾“å‡º live/current.m3u8ï¼ŒIPv4 åœ¨å‰ï¼ŒIPv6 åœ¨å
"""

import requests
import os
import re
from urllib.parse import unquote, urlparse, parse_qs, urlunparse
from datetime import datetime
from ipaddress import ip_address


# ================== Configuration ==================
API_URL = "https://lwydapi.xichongtv.cn/a/appLive/info/35137_b14710553f9b43349f46d33cc2b7fcfd"
PARAMS = {
    'deviceType': '1',
    'centerId': '9',
    'deviceToken': 'beb09666-78c0-4ae8-94e9-b0b4180a31be',
    'latitudeValue': '0',
    'areaId': '907',
    'appCenterId': '907',
    'isTest': '0',
    'longitudeValue': '0',
    'deviceVersionType': 'android',
    'versionCodeGlobal': '5009037'
}
HEADERS = {
    'User-Agent': 'okhttp/3.12.12',
}

# Remote sources
REMOTE_WHITELIST_URL = "https://raw.githubusercontent.com/xichongguo/live-stream/main/whitelist.txt"
TV_M3U_URL = "https://raw.githubusercontent.com/wwb521/live/refs/heads/main/tv.m3u"
RIHOU_URL = "http://rihou.cc:555/gggg.nzk/"
HAIYAN_URL = "https://chuxinya.top/f/AD5QHE/%E6%B5%B7%E7%87%95.txt"  # æ–°å¢ï¼šæµ·ç‡•æº

WHITELIST_TIMEOUT = 15
REQUEST_TIMEOUT = (5, 10)
DEFAULT_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# ---------------- æ–°å¢ï¼šåˆ†ç±»è§„åˆ™ ----------------
CATEGORY_MAP = {
    'å¤®è§†': ['cctv', 'ä¸­å¤®'],
    'å«è§†': [
        'å«è§†', 'æ¹–å—', 'æµ™æ±Ÿ', 'æ±Ÿè‹', 'ä¸œæ–¹', 'åŒ—äº¬', 'å¹¿ä¸œ', 'æ·±åœ³', 'å››å·', 'æ¹–åŒ—', 'è¾½å®',
        'ä¸œå—', 'å¤©æ´¥', 'é‡åº†', 'é»‘é¾™æ±Ÿ', 'å±±ä¸œ', 'å®‰å¾½', 'äº‘å—', 'é™•è¥¿', 'ç”˜è‚ƒ', 'æ–°ç–†',
        'å†…è’™å¤', 'å‰æ—', 'æ²³åŒ—', 'å±±è¥¿', 'å¹¿è¥¿', 'æ±Ÿè¥¿', 'ç¦å»º', 'è´µå·', 'æµ·å—'
    ],
    'åœ°æ–¹': [
        'éƒ½å¸‚', 'æ–°é—»', 'ç»¼åˆ', 'å…¬å…±', 'ç”Ÿæ´»', 'å½±è§†é¢‘é“', 'å½±è§†', 'ç”µè§†å‰§', 'å¨±ä¹',
        'å°‘å„¿', 'å¡é€š', 'ä½“è‚²', 'è´¢ç»', 'çºªå®', 'æ•™è‚²', 'æ°‘ç”Ÿ', 'äº¤é€š', 'æ–‡è‰º', 'éŸ³ä¹',
        'æˆæ›²', 'é«˜å°”å¤«', 'ç½‘çƒ'
    ],
    # --- çœä»½åˆ†ç±» ---
    'å››å·': ['å››å·', 'æˆéƒ½', 'ç»µé˜³', 'å¾·é˜³', 'æ³¸å·', 'å—å……', 'å®œå®¾', 'è¾¾å·', 'å†…æ±Ÿ', 'ä¹å±±', 'è‡ªè´¡', 'æ”€æèŠ±', 'å¹¿å…ƒ', 'é‚å®', 'å·´ä¸­', 'é›…å®‰', 'çœ‰å±±', 'èµ„é˜³'],
    'å¹¿ä¸œ': ['å¹¿ä¸œ', 'å¹¿å·', 'æ·±åœ³', 'ä½›å±±', 'ä¸œè', 'ä¸­å±±', 'ç æµ·', 'æƒ å·', 'æ±Ÿé—¨', 'æ±•å¤´', 'æ¹›æ±Ÿ', 'èŒ‚å', 'è‚‡åº†', 'æ­é˜³', 'æ½®å·', 'æ¸…è¿œ', 'éŸ¶å…³', 'æ±•å°¾', 'é˜³æ±Ÿ', 'æ²³æº'],
    'æ±Ÿè‹': ['æ±Ÿè‹', 'å—äº¬', 'è‹å·', 'æ— é”¡', 'å¸¸å·', 'å¾å·', 'å—é€š', 'æ‰¬å·', 'ç›åŸ', 'æ³°å·', 'é•‡æ±Ÿ', 'æ·®å®‰', 'è¿äº‘æ¸¯', 'å®¿è¿'],
    'æµ™æ±Ÿ': ['æµ™æ±Ÿ', 'æ­å·', 'å®æ³¢', 'æ¸©å·', 'å˜‰å…´', 'ç»å…´', 'é‡‘å', 'å°å·', 'æ¹–å·', 'è¡¢å·', 'ä¸½æ°´', 'èˆŸå±±'],
    'å±±ä¸œ': ['å±±ä¸œ', 'æµå—', 'é’å²›', 'çƒŸå°', 'æ½åŠ', 'æ·„åš', 'ä¸´æ²‚', 'æµå®', 'æ³°å®‰', 'å¨æµ·', 'å¾·å·', 'èŠåŸ', 'æ»¨å·', 'èæ³½', 'æ£åº„'],
    'æ²³å—': ['æ²³å—', 'éƒ‘å·', 'æ´›é˜³', 'å¼€å°', 'æ–°ä¹¡', 'å—é˜³', 'è®¸æ˜Œ', 'å®‰é˜³', 'å•†ä¸˜', 'ä¿¡é˜³', 'å¹³é¡¶å±±', 'å‘¨å£', 'é©»é©¬åº—', 'ç„¦ä½œ', 'æ¿®é˜³', 'æ¼¯æ²³', 'ä¸‰é—¨å³¡', 'é¹¤å£'],
    'æ¹–åŒ—': ['æ¹–åŒ—', 'æ­¦æ±‰', 'å®œæ˜Œ', 'è¥„é˜³', 'é»„å†ˆ', 'è†å·', 'å­æ„Ÿ', 'åå °', 'å’¸å®', 'è†é—¨', 'éšå·', 'æ©æ–½', 'é»„çŸ³', 'é„‚å·'],
    'æ¹–å—': ['æ¹–å—', 'é•¿æ²™', 'æ ªæ´²', 'æ¹˜æ½­', 'è¡¡é˜³', 'å²³é˜³', 'å¸¸å¾·', 'å¼ å®¶ç•Œ', 'æ€€åŒ–', 'éƒ´å·', 'å¨„åº•', 'é‚µé˜³', 'ç›Šé˜³', 'æ°¸å·'],
    'æ²³åŒ—': ['æ²³åŒ—', 'çŸ³å®¶åº„', 'å”å±±', 'ä¿å®š', 'ç§¦çš‡å²›', 'é‚¯éƒ¸', 'é‚¢å°', 'å¼ å®¶å£', 'æ²§å·', 'è¡¡æ°´', 'æ‰¿å¾·'],
    'å®‰å¾½': ['å®‰å¾½', 'åˆè‚¥', 'èŠœæ¹–', 'èšŒåŸ ', 'æ·®å—', 'é©¬éå±±', 'å®‰åº†', 'é˜œé˜³', 'å®¿å·', 'å…­å®‰', 'äº³å·', 'é»„å±±', 'æ»å·', 'æ·®åŒ—', 'å®£åŸ', 'æ± å·'],
    'ç¦å»º': ['ç¦å»º', 'ç¦å·', 'å¦é—¨', 'æ³‰å·', 'æ¼³å·', 'è†ç”°', 'å®å¾·', 'ä¸‰æ˜', 'å—å¹³', 'é¾™å²©'],
    'è¾½å®': ['è¾½å®', 'æ²ˆé˜³', 'å¤§è¿', 'éå±±', 'æŠšé¡º', 'æœ¬æºª', 'ä¸¹ä¸œ', 'é”¦å·', 'è¥å£', 'é˜œæ–°', 'è¾½é˜³', 'é“å²­', 'æœé˜³', 'ç›˜é”¦'],
    'é™•è¥¿': ['é™•è¥¿', 'è¥¿å®‰', 'å®é¸¡', 'å’¸é˜³', 'æ¸­å—', 'æ±‰ä¸­', 'æ¦†æ—', 'å»¶å®‰', 'å®‰åº·', 'å•†æ´›'],
    'å±±è¥¿': ['å±±è¥¿', 'å¤ªåŸ', 'å¤§åŒ', 'é˜³æ³‰', 'é•¿æ²»', 'æ™‹åŸ', 'æœ”å·', 'æ™‹ä¸­', 'è¿åŸ', 'å¿»å·', 'ä¸´æ±¾', 'å•æ¢'],
    'æ±Ÿè¥¿': ['æ±Ÿè¥¿', 'å—æ˜Œ', 'ä¹æ±Ÿ', 'èµ£å·', 'ä¸Šé¥¶', 'å®œæ˜¥', 'å‰å®‰', 'æŠšå·', 'èä¹¡', 'æ–°ä½™', 'é¹°æ½­'],
    'äº‘å—': ['äº‘å—', 'æ˜†æ˜', 'å¤§ç†', 'ä¸½æ±Ÿ', 'ç‰æºª', 'æ›²é–', 'ä¿å±±', 'çº¢æ²³', 'ä¸´æ²§', 'è¥¿åŒç‰ˆçº³', 'æ¥šé›„', 'æ–‡å±±', 'æ™®æ´±', 'æ˜­é€š', 'è¿ªåº†', 'æ€’æ±Ÿ'],
    'è´µå·': ['è´µå·', 'è´µé˜³', 'éµä¹‰', 'å…­ç›˜æ°´', 'å®‰é¡º', 'æ¯•èŠ‚', 'é“œä»', 'é»”ä¸œå—', 'é»”å—', 'é»”è¥¿å—'],
    'å¹¿è¥¿': ['å¹¿è¥¿', 'å—å®', 'æŸ³å·', 'æ¡‚æ—', 'æ¢§å·', 'åŒ—æµ·', 'ç‰æ—', 'é’¦å·', 'è´µæ¸¯', 'ç™¾è‰²', 'è´ºå·', 'æ²³æ± ', 'æ¥å®¾', 'å´‡å·¦'],
    'ç”˜è‚ƒ': ['ç”˜è‚ƒ', 'å…°å·', 'å¤©æ°´', 'ç™½é“¶', 'åº†é˜³', 'å®šè¥¿', 'æ­¦å¨', 'å¼ æ–', 'å¹³å‡‰', 'é…’æ³‰', 'é™‡å—', 'ä¸´å¤', 'ç”˜å—'],
    'æ–°ç–†': ['æ–°ç–†', 'ä¹Œé²æœ¨é½', 'å…‹æ‹‰ç›ä¾', 'åé²ç•ª', 'å“ˆå¯†', 'åº“å°”å‹’', 'é˜¿å…‹è‹', 'å–€ä»€', 'å’Œç”°', 'ä¼Šå®', 'çŸ³æ²³å­'],
    'å†…è’™å¤': ['å†…è’™å¤', 'å‘¼å’Œæµ©ç‰¹', 'åŒ…å¤´', 'èµ¤å³°', 'é€šè¾½', 'é„‚å°”å¤šæ–¯', 'å‘¼ä¼¦è´å°”', 'å·´å½¦æ·–å°”', 'ä¹Œå…°å¯Ÿå¸ƒ', 'é”¡æ—éƒ­å‹’', 'å…´å®‰ç›Ÿ'],
    'å‰æ—': ['å‰æ—', 'é•¿æ˜¥', 'å‰æ—å¸‚', 'å››å¹³', 'è¾½æº', 'é€šåŒ–', 'ç™½å±±', 'æ¾åŸ', 'ç™½åŸ'],
    'é»‘é¾™æ±Ÿ': ['é»‘é¾™æ±Ÿ', 'å“ˆå°”æ»¨', 'é½é½å“ˆå°”', 'ç‰¡ä¸¹æ±Ÿ', 'ä½³æœ¨æ–¯', 'å¤§åº†', 'ç»¥åŒ–', 'é¹¤å²—', 'é¸¡è¥¿', 'åŒé¸­å±±', 'ä¸ƒå°æ²³', 'é»‘æ²³', 'å¤§å…´å®‰å²­'],
    'æµ·å—': ['æµ·å—', 'æµ·å£', 'ä¸‰äºš', 'å„‹å·', 'ç¼æµ·', 'ä¸‡å®', 'ä¸œæ–¹', 'äº”æŒ‡å±±', 'æ–‡æ˜Œ', 'ä¹ä¸œ', 'æ¾„è¿ˆ', 'å®šå®‰'],
    'é¦™æ¸¯': ['é¦™æ¸¯', 'HK', 'RTHK', 'TVB', 'ATV'],
    'æ¾³é—¨': ['æ¾³é—¨', 'Macao', 'TDM'],
    'å°æ¹¾': ['å°æ¹¾', 'Taiwan', 'å°è¦–', 'ä¸­è¦–', 'è¯è¦–', 'æ°‘è¦–', 'å…¬è¦–', 'TVBS', 'ä¸‰ç«‹', 'ä¸œæ£®', 'ä¸­å¤©']
}

# ---------------- æ–°å¢ï¼šå›½å¤–å…³é”®è¯è¿‡æ»¤ ----------------
FOREIGN_KEYWORDS = {
    'cnn', 'bbc', 'fox', 'abc', 'nbc', 'cbc', 'pbs', 'sky', 'disney',
    'nick', 'mtv', 'espn', 'hbo', 'paramount', 'warner', 'pluto',
    'france', 'deutsch', 'german', 'italia', 'spain', 'espanol',
    'japan', 'tokyo', 'nhk', 'korea', 'seoul', 'sbs', 'kbs', 'mbc',
    'india', 'bollywood', 'russia', 'moscow', 'turkey', 'egypt',
    'arab', 'qatar', 'dubai', 'australia', 'sydney', 'canada',
    'mexico', 'brazil', 'argentina', 'chile', 'south africa',
    'singapore', 'malaysia', 'thailand', 'vietnam', 'philippines', 'indonesia',
    'pakistan', 'iran', 'iraq', 'israel', 'sweden', 'norway', 'denmark',
    'switzerland', 'austria', 'belgium', 'netherlands', 'poland', 'ukraine',
    'greece', 'portugal', 'finland', 'ireland', 'new zealand'
}

ALLOWED_FOREIGN = {'é¦™æ¸¯', 'æ¾³é—¨', 'å°æ¹¾', 'HK', 'Macao', 'Taiwan', 'TVB', 'ATV', 'TDM', 'å°è¦–', 'ä¸­è¦–', 'è¯è¦–', 'æ°‘è¦–', 'å…¬è¦–'}


# ================== Utility Functions ==================
def is_foreign_channel(name):
    """åˆ¤æ–­æ˜¯å¦ä¸ºå›½å¤–é¢‘é“ï¼ˆæ’é™¤æ¸¯æ¾³å°ï¼‰"""
    name_lower = name.lower()
    for allowed in ALLOWED_FOREIGN:
        if allowed in name:
            return False
    for keyword in FOREIGN_KEYWORDS:
        if keyword in name_lower:
            return True
    return False


def extract_ip_from_url(url):
    """ä» URL ä¸­æå– IP åœ°å€"""
    try:
        parsed = urlparse(url)
        hostname = parsed.hostname
        if not hostname:
            return None
        return ip_address(hostname)
    except:
        return None


def is_ipv4(ip):
    """åˆ¤æ–­æ˜¯å¦ä¸º IPv4 åœ°å€"""
    return ip.version == 4 if ip else False


def is_ipv6(ip):
    """åˆ¤æ–­æ˜¯å¦ä¸º IPv6 åœ°å€"""
    return ip.version == 6 if ip else False


def is_valid_url(url):
    """å¿«é€Ÿæ£€æµ‹ URL æ˜¯å¦å¯ç”¨"""
    try:
        # ä½¿ç”¨ HEAD è¯·æ±‚å¿«é€Ÿæ£€æµ‹
        response = requests.head(url, timeout=3, headers=DEFAULT_HEADERS, allow_redirects=True, stream=True)
        return response.status_code < 400
    except:
        try:
            # å¤‡ç”¨ï¼šGET è¯·æ±‚ï¼Œä½†åªè¯»å–å°‘é‡æ•°æ®
            response = requests.get(url, timeout=5, headers=DEFAULT_HEADERS, stream=True)
            response.raw.read(1)
            response.close()
            return True
        except:
            return False


def normalize_url(url):
    """Remove tracking/query params for deduplication."""
    try:
        parsed = urlparse(url.lower())
        safe_params = {}
        unsafe_keys = {'token', 't', 'ts', 'sign', 'auth_key', 'verify', 'session', 'key', 'pwd', 'stb', 'icpid', 'RTS', 'from', 'hms_devid', 'online', 'vqe'}
        for k, v in parse_qs(parsed.query).items():
            if k.lower() not in unsafe_keys:
                if v and v[0]:
                    safe_params[k] = v[0]
        new_query = '&'.join(f"{k}={v}" for k, v in safe_params.items())
        return urlunparse(parsed._replace(query=new_query))
    except:
        return url.lower().split('?')[0]


def categorize_channel(name):
    """Auto categorize channel by name."""
    name_lower = name.lower()
    for category, keywords in CATEGORY_MAP.items():
        for kw in keywords:
            if kw.lower() in name_lower:
                return category
    return "å…¶ä»–"


def load_whitelist_from_remote():
    """Load whitelist -> æœ¬åœ°èŠ‚ç›® (trusted, no test, keep all)"""
    print(f"ğŸ‘‰ Loading trusted whitelist: {REMOTE_WHITELIST_URL}")
    try:
        response = requests.get(REMOTE_WHITELIST_URL, timeout=WHITELIST_TIMEOUT)
        response.raise_for_status()
        lines = response.text.strip().splitlines()
        channels = []

        for line in lines:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            parts = [p.strip() for p in line.split(",", 1)]
            if len(parts) < 2:
                continue
            name, url = parts[0], parts[1]
            if not name or not url or not url.startswith(("http://", "https://")):
                continue
            if is_foreign_channel(name):
                print(f"ğŸŒ Skipped foreign (whitelist): {name}")
                continue
            channels.append((name, url, "æœ¬åœ°èŠ‚ç›®", True))
            print(f"  â• Whitelist: {name} -> æœ¬åœ°èŠ‚ç›® (trusted, no test)")
        print(f"âœ… Loaded {len(channels)} from whitelist (no test)")
        return channels
    except Exception as e:
        print(f"âŒ Load whitelist failed: {e}")
        return []


def load_tv_m3u():
    """Load tv.m3u (priority source, with testing)"""
    print(f"ğŸ‘‰ Loading priority source: {TV_M3U_URL}")
    try:
        response = requests.get(TV_M3U_URL, timeout=WHITELIST_TIMEOUT, headers=DEFAULT_HEADERS)
        response.raise_for_status()
        lines = response.text.strip().splitlines()
        channels = []
        current_name = None

        for line in lines:
            line = line.strip()
            if line.startswith("#EXTINF"):
                try:
                    name_part = line.split(",", 1)
                    if len(name_part) > 1:
                        current_name = name_part[1].strip()
                except:
                    current_name = "Unknown"
            elif line.startswith("http"):
                if current_name and line.startswith(("http://", "https://")):
                    if is_foreign_channel(current_name):
                        print(f"ğŸŒ Skipped foreign (tv.m3u): {current_name}")
                    else:
                        category = categorize_channel(current_name)
                        channels.append((current_name, line, category, False))
                        print(f"  â• tv.m3u: {current_name} -> {category}")
                current_name = None
        print(f"âœ… Loaded {len(channels)} from tv.m3u")
        return channels
    except Exception as e:
        print(f"âŒ Failed to load tv.m3u: {e}")
        return []


def load_rihou_source():
    """
    Load source from http://rihou.cc:555/gggg.nzk/
    - Skip categories: ä¸­è¶…èµ›è¯„, æ¹˜è¶…èµ›è¯„, è‹è¶…èµ›è¯„, è‹±è¶…ç²¤è¯„
    - Extract 'èµ›äº‹å’ªå’•' to be moved to end
    """
    print(f"ğŸ‘‰ Loading source: {RIHOU_URL}")
    try:
        response = requests.get(RIHOU_URL, timeout=WHITELIST_TIMEOUT, headers=DEFAULT_HEADERS)
        response.raise_for_status()
        content = response.text.strip()
        lines = content.splitlines()

        channels = []
        saishi_migu_channels = []
        current_category = None
        skip_categories = {'ä¸­è¶…èµ›è¯„', 'æ¹˜è¶…èµ›è¯„', 'è‹è¶…èµ›è¯„', 'è‹±è¶…ç²¤è¯„'}

        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            if line.endswith(',#genre#'):
                category_name = line.split(',', 1)[0].strip()
                current_category = category_name
                continue

            if ',' not in line:
                continue
            parts = line.split(',', 1)
            if len(parts) != 2:
                continue

            name = parts[0].strip()
            url = parts[1].strip()

            if url.startswith('video://'):
                url = url[8:]

            if is_foreign_channel(name):
                print(f"ğŸŒ Skipped foreign (rihou): {name}")
                continue

            if current_category in skip_categories:
                continue

            if current_category == 'èµ›äº‹å’ªå’•':
                category = 'èµ›äº‹å’ªå’•'
                saishi_migu_channels.append((name, url, category, False))
                print(f"  â• èµ›äº‹å’ªå’•: {name} -> {category} (will move to end)")
                continue

            category = categorize_channel(name)
            channels.append((name, url, category, False))
            print(f"  â• rihou: {name} -> {category}")

        print(f"âœ… Loaded {len(channels)} from rihou (excl. èµ›äº‹å’ªå’•), {len(saishi_migu_channels)} èµ›äº‹å’ªå’• channels")
        return channels, saishi_migu_channels

    except Exception as e:
        print(f"âŒ Load rihou source failed: {e}")
        return [], []


def load_haiyan_source():
    """Load æµ·ç‡•.txt source"""
    print(f"ğŸ‘‰ Loading æµ·ç‡•æº: {HAIYAN_URL}")
    try:
        response = requests.get(HAIYAN_URL, timeout=WHITELIST_TIMEOUT, headers=DEFAULT_HEADERS)
        response.raise_for_status()
        content = response.text.strip()
        lines = content.splitlines()

        channels = []
        current_category = None

        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            if line.endswith(',#genre#'):
                current_category = line.split(',', 1)[0].strip()
                continue

            if ',' not in line:
                continue
            parts = line.split(',', 1)
            if len(parts) != 2:
                continue

            name = parts[0].strip()
            url = parts[1].strip()

            if url.startswith('video://'):
                url = url[8:]

            if is_foreign_channel(name):
                print(f"ğŸŒ Skipped foreign (æµ·ç‡•): {name}")
                continue

            category = categorize_channel(name)
            channels.append((name, url, category, False))
            print(f"  â• æµ·ç‡•: {name} -> {category}")

        print(f"âœ… Loaded {len(channels)} from æµ·ç‡•æº")
        return channels

    except Exception as e:
        print(f"âŒ Load æµ·ç‡•æº failed: {e}")
        return []


def get_dynamic_stream():
    """Fetch dynamic stream from API â€” skip testing."""
    print("ğŸ‘‰ Fetching dynamic stream from API...")
    try:
        response = requests.get(API_URL, params=PARAMS, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        if 'data' in data and 'm3u8Url' in data['data']:
            url = data['data']['m3u8Url']
            print(f"âœ… Dynamic stream added (no test): {url}")
            return ("è¥¿å……ç»¼åˆ", url, "æœ¬åœ°èŠ‚ç›®", True)
        else:
            print("âŒ m3u8Url not found in API response")
    except Exception as e:
        print(f"âŒ API request failed: {e}")
    return None


def merge_channels_by_name(channels):
    """
    åˆå¹¶åŒåé¢‘é“ï¼Œä¼˜å…ˆä¿ç•™ IPv4 æºï¼Œå»é™¤å¤±æ•ˆæº
    è¾“å‡ºï¼šIPv4 åœ¨å‰ï¼ŒIPv6 åœ¨å
    """
    print("ğŸ”„ å¼€å§‹åˆå¹¶åŒåé¢‘é“ï¼Œä¼˜å…ˆä¿ç•™ IPv4 æº...")
    from collections import defaultdict

    # æŒ‰åç§°åˆ†ç»„
    grouped = defaultdict(list)
    for item in channels:
        name, url, group, is_trusted = item
        grouped[name].append(item)

    merged = []

    for name, items in grouped.items():
        # å…ˆè¿‡æ»¤å¤±æ•ˆæºï¼ˆéç™½åå•ï¼‰
        valid_items = []
        for item in items:
            _, url, _, is_trusted = item
            if is_trusted or is_valid_url(url):
                valid_items.append(item)
            else:
                print(f"ğŸ’€ å¤±æ•ˆæºå·²ç§»é™¤: {name} -> {url}")

        if not valid_items:
            continue

        # æå– IP å¹¶æ’åºï¼šIPv4 åœ¨å‰ï¼ŒIPv6 åœ¨å
        def sort_key(item):
            _, url, _, _ = item
            ip = extract_ip_from_url(url)
            return (0 if is_ipv4(ip) else 1, str(ip))  # IPv4 ä¼˜å…ˆ

        valid_items.sort(key=sort_key)

        # åªä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆå³æœ€ä¼˜çš„ IPv4 æˆ– IPv6ï¼‰
        best_item = valid_items[0]
        merged.append(best_item)

        # å¦‚æœæœ‰å¤šä¸ªï¼Œæç¤º
        if len(valid_items) > 1:
            print(f"ğŸ” åŒåé¢‘é“åˆå¹¶: {name} -> ä¿ç•™ IPv4 æº")

    print(f"âœ… åˆå¹¶å®Œæˆï¼Œå…±ä¿ç•™ {len(merged)} ä¸ªé¢‘é“")
    return merged


def generate_m3u8_content(channels):
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    lines = [
        "#EXTM3U",
        f"# Generated at: {now}",
        "x-tvg-url=\"https://epg.51zmt.top/xmltv.xml\""
    ]

    for name, url, group, _ in channels:
        lines.append(f'#EXTINF:-1 tvg-name="{name}" group-title="{group}",{name}')
        lines.append(url)

    return "\n".join(lines) + "\n"


def main():
    print("ğŸš€ Starting playlist generation...")
    os.makedirs('live', exist_ok=True)
    print("ğŸ“ Ensured live/ directory")

    all_channels = []
    saishi_migu_list = []

    # 1. è·å–åŠ¨æ€æµï¼ˆå¦‚è¥¿å……ç»¼åˆï¼‰
    dynamic_item = get_dynamic_stream()
    if dynamic_item:
        all_channels.append(dynamic_item)

    # 2. åŠ è½½ tv.m3uï¼ˆä¼˜å…ˆï¼‰
    all_channels.extend(load_tv_m3u())

    # 3. åŠ è½½ç™½åå•ï¼ˆå…æ£€ï¼‰
    all_channels.extend(load_whitelist_from_remote())

    # 4. åŠ è½½ rihou æº
    rihou_normal, rihou_saishi_migu = load_rihou_source()
    all_channels.extend(rihou_normal)
    saishi_migu_list.extend(rihou_saishi_migu)

    # 5. æ–°å¢ï¼šåŠ è½½æµ·ç‡•æº
    haiyan_channels = load_haiyan_source()
    all_channels.extend(haiyan_channels)

    print(f"ğŸ“¥ Total raw streams: {len(all_channels)}")

    # åˆå¹¶åŒåé¢‘é“ï¼Œå»å¤±æ•ˆï¼ŒIPv4 ä¼˜å…ˆ
    unique_channels = merge_channels_by_name(all_channels)

    # å†æ¬¡è¿‡æ»¤å›½å¤–ï¼ˆç¡®ä¿å®‰å…¨ï¼‰
    final_main = [item for item in unique_channels if not is_foreign_channel(item[0])]

    # æ·»åŠ  'èµ›äº‹å’ªå’•' åˆ°æœ«å°¾
    final_with_saishi_migu = final_main + saishi_migu_list

    print(f"âœ… Final playlist size: {len(final_with_saishi_migu)} channels (after adding èµ›äº‹å’ªå’•)")

    # ç”Ÿæˆ M3U8
    m3u8_content = generate_m3u8_content(final_with_saishi_migu)

    # å†™å…¥æ–‡ä»¶
    output_path = 'live/current.m3u8'
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(m3u8_content)
        print(f"ğŸ‰ Successfully generated: {output_path}")
    except Exception as e:
        print(f"âŒ Write failed: {e}")
        return

    if not os.path.exists('.nojekyll'):
        open('.nojekyll', 'w').close()
        print("ğŸ“„ Created .nojekyll")

    print("âœ… All tasks completed!")


if __name__ == "__main__":
    main()
